{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classifier Evaluation\n",
    "This notebook aims at assessing the performance of a binary classifier, calculating some useful evaluation metrics.\n",
    "\n",
    "*Assumptions:*\n",
    "- The predictions from the binary classifier are stored in the format adopted in the inference step implemented in `steps/infer.py`. This format expects the output classifier confidence score in a simple *numpy* array stored in an `npy` file, named after the image.\n",
    "- Similarly, since at the end of this notebook CAMs will be visualized, these are assumed to be saved in a similar format. The CAM should be a *numpy* 2D array stored in a `npy` file, named after the image. This format is, again, the one adopted in the inference step implemented in `steps/infer.py`.\n",
    "- Ground truth labels are assumed to be located in a JSON file describing a dictionary with a specific structure, described below. This is the same structure adopted by the [AerialWaste](https://aerialwaste.org) dataset.\n",
    "\n",
    "*Assumed dataset structure:*\n",
    "```python\n",
    "{\n",
    "   \"categories\":[{\"id\": 1, \"name\": \"candidate_site\"}],\n",
    "   \"images\":[\n",
    "      {\n",
    "         \"id\":1,\n",
    "         \"categories\":[1],\n",
    "         \"file_name\":\"1.png\",\n",
    "         \"height\":1000,\n",
    "         \"width\":1000,\n",
    "         ...\n",
    "      },\n",
    "      ...\n",
    "   ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import (roc_curve, auc, precision_recall_curve, \n",
    "                             average_precision_score, confusion_matrix, \n",
    "                             f1_score, precision_score, accuracy_score, recall_score)\n",
    "from tqdm.notebook import tqdm\n",
    "from misc.utils import onehot, CAMVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "ds_dir = \"AerialWaste3.0\"               # Dataset source directory\n",
    "img_dir = os.path.join(ds_dir, \"images\")                # Directory with dataset images [needed for CAM visualization]\n",
    "eval_json = os.path.join(ds_dir, \"testing-binary.json\")     # Path to JSON file with GT information of images to include in evaluation\n",
    "class_out_dir = \"test-experiment\"   # Path to directory associated to the binary classifier experiment [defined for convenience]\n",
    "pred_dir = os.path.join(class_out_dir, \"predictions\")       # Path to directory with the classifier output predictions\n",
    "cams_dir = os.path.join(class_out_dir, \"cams\")              # Path to directory with the classifier output CAMs\n",
    "# Check file/directory existence\n",
    "assert os.path.isdir(img_dir)\n",
    "assert os.path.isfile(eval_json)\n",
    "assert os.path.isdir(pred_dir)\n",
    "assert os.path.isdir(cams_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "# Classification threshold\n",
    "threshold_type = 'custom'   # 'custom' or 'best_f1'. Use the former to set a user-defined threshold, the latter to dynamically select the threshold yielding the best F1 score on the current set.\n",
    "threshold_value = .5        # User-defined threshold value. Ignored if 'threshold_type' is 'best_f1'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load ground truth\n",
    "Load to a dedicated variable the content of the JSON file with ground truth information over the dataset to use for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(eval_json) as file:\n",
    "    gt_content = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories\n",
    "Store information about target category from the dataset. This will be used to support execution of some of the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = gt_content[\"categories\"]               # Store category information in a dedicated variable for convenience\n",
    "num_cats = len(categories)                          # Number of categories\n",
    "cat_names = [cat[\"name\"] for cat in categories]     # Category names\n",
    "cat_ids = [cat[\"id\"] for cat in categories]         # Category IDs\n",
    "# Check number of categories is 1 --> verifies used dataset is suitable for binary classification\n",
    "assert num_cats == 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Predictions\n",
    "Load the predictions for each image. Predictions will be stored in a dictionary with as keys the names of the image files, and as values the classification scores. Such a dictionary is exemplified below.\n",
    "```python\n",
    "{\n",
    "   \"1.png\": 0.996947705745697,\n",
    "   \"2.png\": 0.012907988391816616,\n",
    "   \"3.png\": 0.5127884149551392,\n",
    "    ...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize empty dictionary to store predictions\n",
    "predictions = dict()\n",
    "# For each image, save necessary information in the newly-created dictionary\n",
    "for image in tqdm(gt_content[\"images\"], leave=False):\n",
    "    fn = image[\"file_name\"]             # Store image file name for convenience\n",
    "    # Search and load associated prediction in dedicated folder\n",
    "    pred_file = os.path.join(pred_dir, '.'.join(fn.split('.')[:-1])+'.npy')     # Handles also the case where '.' is found in the file name, before extension\n",
    "    assert os.path.isfile(pred_file)    # Check file existence\n",
    "    predictions[fn] = np.load(pred_file)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute `y_score`, `y_true` \n",
    "Extract information from predictions and ground truth to compose 2 arrays which will be crucial to compute all the metrics described in the following sections:\n",
    "* `y_score`: classification scores output from the model. 1D array of length `num_images`.\n",
    "* `y_true`: ground truth target values. 1D array of length `num_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = np.array(list(predictions.values()))\n",
    "y_true = np.array([onehot(image[\"categories\"], cat_ids) for image in gt_content[\"images\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Metrics\n",
    "In the context of a binary classification task, the terms *positive* and *negative* are used to describe the classifier predictions, whereas the terms *true* and *false* refer to whether the prediction corresponds to the external judgment, i.e., the ground truth *observation*. Given these definitions, it is possible to formulate the following table:\n",
    "\n",
    "<table>\n",
    "   <tr>\n",
    "      <th></th>\n",
    "      <th colspan=2, style=\"text-align: center\">Actual class (observation)</th>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <th rowspan=\"2\">Predicted class (expectation)</th>\n",
    "      <td>True Positive (TP) Correct result</td>\n",
    "      <td>False Positive (FP) Unexpected result</td>\n",
    "   </tr>\n",
    "   <tr>\n",
    "      <td>False Negative (FN) Missing result</td>\n",
    "      <td>True Negative (TN) Correct absence of result</td>\n",
    "   </tr>\n",
    "</table>\n",
    "\n",
    "* **True Positives (TP)**: samples for which the model correctly predicted the positive class (e.g. the model inferred that a particular email message was spam, and it really was spam).\n",
    "* **True Negatives (TN)**: samples for which which the model correctly predicted the negative class (e.g. the model inferred that a particular email message was not spam, and that email message really was not spam).\n",
    "* **False Positives (FP)**: samples for which the model mistakenly predicted the positive class (e.g. the model inferred that a particular email message was spam (the positive class), but that email message was actually not spam).\n",
    "* **False Negatives (FN)**: samples for which the model mistakenly predicted the negative class (e.g. the model inferred that a particular email message was not spam (the negative class), but that email message actually was spam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification score distribution\n",
    "This section introduces the first plot to study the performance of a given model: a histogram describing the distribution of the classification scores output by the model across the whole range of possible output scores, i.e., the [0,1] range. To show this distribution, the histogram is composed by splitting the considered range into a set of bins, each collecting a portion of the classification scores. If the considered model were perfect and all its predictions were correct, the histogram would collect all negative samples in the first bin, i.e., the one closest to 0, and all positive samples in the last bin, i.e., the one closest to 1.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bins = np.histogram(list(predictions.values()), bins=10)\n",
    "plt.hist(bins[:-1], bins, weights=counts, edgecolor='black')\n",
    "plt.xlim([0,1])\n",
    "plt.xlabel('Classification score', fontsize=14)\n",
    "plt.ylabel('Number of samples', fontsize=14)\n",
    "plt.title('Classification score distribution', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve and AUC\n",
    "The **ROC curve** (Receiver Operating Characteristic curve) is a plot describing the performance of a classification model across all possible thresholds. To plot this curve, two specific metrics are needed:\n",
    "\n",
    "* **True Positive Rate (TPR) / Recall**:\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "* **False Positive Rate (FPR) / Fall-Out**: \n",
    "\n",
    "$$FPR = \\frac{FP}{FP + TN}$$\n",
    "\n",
    "Given these metrics, the ROC curve plots the TPR (y-axis) against the FPR (x-axis), after computing these values for all the available thresholds. The choice of this factor plays a crucial role, since lowering the classification threshold implies classifying more items as positive, thus increasing both False Positives and True Positives, which appear in the formulae reported above. Based on its ROC curve, a good classifier could be identified when the curve remains as far as possible from the diagonal toward the top-left corner, dashed in the following plot. As a final reference, the best model, i.e., the model always making correct predictions, should have its ROC passing from the (0,1) point and appearing as a horizontal line a the value 1.\n",
    "\n",
    "The **AUC**, which stands for \"Area Under the ROC Curve\", is a metric strictly correlated to the ROC curve described above. This metric measures the area underneath the entire ROC curve in the rectangle with opposite vertices in (0,0) and (1,1). A possible iterpretation of this metric is the probability that the model ranks a random positive example higher than a random negative example.\n",
    "AUC cam acquire values in the range between 0 and 1, with models making only wrong predictions having an AUC of 0, whereas models making 100% correct prediction having an AUC of 1.\n",
    "\n",
    "AUC is a metric with 2 crucial properties, which make it suitable for comparing different models:\n",
    "* **Threshold-invariance**: AUC provides a robust measure of the model performance by evaluating predictions across all possible thresholds;\n",
    "* **Scale-invariance**: AUC evaluates predictions based on their ranking, rather than on their absolute values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholdsroc = roc_curve(y_true, y_score)\n",
    "aucvalue = auc(fpr, tpr)\n",
    "plt.plot(fpr, tpr, linewidth=2, label=None)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.axis([0, 1, 0, 1])                                \n",
    "plt.title(f'ROC Curve, AUC: {aucvalue:.4f}', fontsize=18)    \n",
    "plt.xlabel('False Positive Rate (Fall-Out)', fontsize=14)\n",
    "plt.ylabel('True Positive Rate (Recall)', fontsize=14)\n",
    "plt.grid(True)                            \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall curve and Average Precision\n",
    "The ROC and AUC metrics described above are particularly useful when dealing with (roughly-)balanced datasets. In case the dataset is imbalanced, the Precision-Recall curve might be more effective. In order to plot this curve, two different metrics are needed:\n",
    "\n",
    "* **Precision**: this metric is the ratio of the correctly predicted positives over all the samples predicted as positives. Therefore, this metric identifies the frequency with which the model was correct when predicting the positive class. It can also be seen as the classifier ability not to label as positive a sample that is truly negative.\n",
    "\n",
    "$$P = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "* **Recall**: it is the ratio of correctly predicted positive instances over the number of instances which are actually positive. It describes, out of all the possible correct positive labels, how many the model correctly identified. It can also be seen as the ability of the classifier to find all the positive samples.\n",
    "\n",
    "$$R = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "Precision and Recall raise the key issue of Precision-Recall tradeoff, which is also strictly related to the choice of the threshold value to consider a prediction positive. By raising the threshold, indeed, the number of positives diminishes, thus favoring Precision but lowering Recall. On the contrary, lowering the threshold implies raising the number of positive predictions, thus favoring Recall at the expenses of Precision\n",
    "\n",
    "The Precision-Recall curve plots a comparison of these 2 metrics after collecting their value at different classification thresholds. In addition, the curve can also be summarized by another specific metric:\n",
    "\n",
    "* **(Weighted) Average Precision**: this metric is a mean of the Precision values at each threshold, weighted by the increase in Recall from the previous threshold. Since the metric can be considered to resume the P-R curve, the metric can also be adopted to compare different models. Please, notice that sometimes the term AP refers to the simple mean of Precision values at all possible thresholds. In this notebook, the reference to AP always refers to its weighted counterpart, which considers both Precision and Recall.\n",
    "\n",
    "$$AP = \\frac{1}{n}\\sum_{i=1}^{n} (R_i-R_{i-1})P_i$$\n",
    "\n",
    "where *i* is the threshold index and *n* is the total number of considered thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precs, recs, thresholds = precision_recall_curve(y_true, y_score)\n",
    "ap = average_precision_score(y_true, y_score)\n",
    "\n",
    "plt.plot(recs, precs, linewidth=2)\n",
    "plt.ylim([0, 1])\n",
    "plt.xlim([0, 1])\n",
    "plt.title(f'Precision-Recall curve, AP: {ap:.4f}', fontsize=18)    \n",
    "plt.xlabel('Recall', fontsize=14)\n",
    "plt.ylabel('Precision', fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a useful support to the threshold study, the following cell plots the 3 main metrics at the various threshold values. For a description of F1-Score, see the cells below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1,3, figsize=(24,6))\n",
    "metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "for i, ax in enumerate(axes):\n",
    "    match metrics[i]:\n",
    "        case 'Precision':\n",
    "            ax.plot(thresholds, precs[:-1], linewidth=2)\n",
    "        case 'Recall':\n",
    "            ax.plot(thresholds, recs[:-1], linewidth=2)\n",
    "        case 'F1-Score':\n",
    "            ax.plot(thresholds, (2*precs[:-1]*recs[:-1])/(precs[:-1]+recs[:-1]), linewidth=2)\n",
    "    ax.set_ylim([0, 1])\n",
    "    ax.set_xlim([0, 1])\n",
    "    ax.set_title(f'{metrics[i]} by threshold', fontsize=18)    \n",
    "    ax.set_xlabel('Thresholds', fontsize=14)\n",
    "    ax.set_ylabel(metrics[i], fontsize=14)\n",
    "    ax.grid(True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification threshold selection\n",
    "All the metrics described and computed in the following sections require a specific threshold to be selected, in order to allow the computation of TP, TN, FP and FN by setting a cut-off confidence for classifying an image as positive. This section focuses on selecting such threshold.\n",
    "\n",
    "This notebook supports 2 types of thresholds, for which a specific parameter must be adjusted in the first cells of the notebook itself. Based on the value of the `threshold_type` parameter, the adopted threshold could be:\n",
    "- `custom`, a user-defined fixed threshold, whose value should be specified above in the variable `threshold_value`.\n",
    "- `best_f1`, a dynamically defined value which maximizes the F1-Score on the dataset used for evaluation. For more information about the F1-Score, please, see the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select threshold\n",
    "if threshold_type == 'best_f1':\n",
    "    # Compute F1 scores\n",
    "    f1s = 2*precs*recs/(precs+recs)\n",
    "    threshold = thresholds[np.argmax(f1s)]\n",
    "elif threshold_type == 'custom':\n",
    "    threshold = threshold_value\n",
    "else:\n",
    "    raise ValueError('Not supported threshold type passed as parameter.')\n",
    "    \n",
    "# Print summary of selected threshold\n",
    "print(f\"Classification threshold: {threshold:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute `y_pred`\n",
    "Compute predictions by filtering scores on the just-selected. This process of another array, similar to `y_score` and `y_true` computed before: \n",
    "* `y_pred`: targets as predicted by the classifier. 1D array of length `num_images`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = (y_score > threshold)*1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "The confusion matrix is a table to visualize the distribution of samples across the predicted and expected categories. The table has 2 dimensions ('Predicted' and 'Actual') with identical sets of classes, i.e., those considered in the specific problem. In a binary classification task, the classes are Positive (1) and Negative (0), thus allowing the creation of 4 cells at their intersection. In these cells lie the numbers of TP, TN, FP and FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# Plot confusion matrix\n",
    "texts = [[\"TN\", \"FP\"], [\"FN\", \"TP\"]]\n",
    "ticks = [0, 1]\n",
    "plt.title(f\"Confusion Matrix\", fontsize=18)\n",
    "plt.xlabel(\"Predicted\", fontsize=14)\n",
    "plt.ylabel(\"Actual\", fontsize=14)\n",
    "plt.xticks(ticks, fontsize=12)\n",
    "plt.yticks(ticks, fontsize=12)\n",
    "thresh = cm.max() / 2\n",
    "plt.imshow(cm, cmap=plt.cm.Blues, interpolation=\"nearest\")\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        text = texts[i][j] + \"\\n\\n\" + str(cm[i, j])\n",
    "        color = \"white\" if cm[i, j] > thresh else \"black\"\n",
    "        alignment = \"center\"\n",
    "        plt.text(j, i, text, \n",
    "                 horizontalalignment=alignment,\n",
    "                 verticalalignment=alignment, \n",
    "                 color=color, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy, Precision, Recall and F1 \n",
    "\n",
    "This section computes 4 of the most relevant metrics for any classification problem.\n",
    "\n",
    "* **Accuracy**: fraction of correct predictions.\n",
    "\n",
    "$$accuracy = \\frac{TP + TN}{TP + TN + FP +FN}$$\n",
    "\n",
    "* **Precision**: accuracy of positive predictions. Identifies the frequency with which a model was correct when predicting the positive class and can be seen as the ability of the classifier not to label as positive a sample that is negative.\n",
    "\n",
    "$$precision = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "* **Recall**: ratio of positive instances that are correctly detected by the classfier. It describes how many of all the actual positive samples in the dataset are correctly identified by model and can also be seen as the ability of the classifier to find all the positive samples.\n",
    "\n",
    "$$recall = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "* **F1 Score**: the harmonic mean of precision and recall, thus combining these 2 metrics into a single one.\n",
    "\n",
    "$$F_1 = \\frac{2}{\\frac{1}{recall} + \\frac{1}{precision}} = 2 * \\frac{precision*recall}{precision+recall} = \\frac{TP}{TP + \\frac{FN + FP}{2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "prec = precision_score(y_true, y_pred)\n",
    "rec = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "# Print metrics\n",
    "print(f\"Accuracy: {acc*100:.2f}%\")\n",
    "print(f\"Precision: {prec*100:.2f}%\")\n",
    "print(f\"Recall: {rec*100:.2f}%\")\n",
    "print(f\"F1 Score: {f1*100:.2f}%\")\n",
    "print(f\"Average Precision (AP): {ap*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize CAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate images into True/False Positives/Negatives\n",
    "tps = [img for img in gt_content[\"images\"] if 1 in img[\"categories\"] and (threshold < predictions[img[\"file_name\"]])]\n",
    "fps = [img for img in gt_content[\"images\"] if 1 not in img[\"categories\"] and (threshold < predictions[img[\"file_name\"]])]\n",
    "tns = [img for img in gt_content[\"images\"] if 1 not in img[\"categories\"] and (threshold > predictions[img[\"file_name\"]])]\n",
    "fns = [img for img in gt_content[\"images\"] if 1 in img[\"categories\"] and (threshold > predictions[img[\"file_name\"]])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select images to see CAMs of\n",
    "images = [img for img in gt_content[\"images\"]]\n",
    "image_paths = [os.path.join(img_dir, img[\"file_name\"]) for img in images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract list of category names (actually, list with a single cat)\n",
    "cats_names = [gt_content[\"categories\"][0][\"name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualizer = CAMVisualizer(image_paths, \n",
    "                           cat_names=cat_names, \n",
    "                           cams_dir=cams_dir,\n",
    "                           preds_dir=pred_dir,\n",
    "                           plot_cols=2)\n",
    "visualizer.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fedeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
